<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Crawl.js - Dezentralized crawler for Node.js</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Cédric Reginster">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/solarized.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">

        <!-- Crawl.js -->
        <section>
          <h1>Crawl.js</h1>
          <h3>A decentralized crawler for Node.js</h3>
          <p>Author: Cédric Reginster</p>
          <p>Superviser: Prof. Pascal Felber</p>
          <img width="20%" src="img/icon.png" alt="crawl.js logo">
        </section>

        <section>
          <h2>Goals</h2>
          <ul>
            <li>Design and develop a decentralized service using the node.js technology to <b>efficiently</b> crawl and index the Web</li>
            <li>
              <b>Efficiently:</b>
              <ul>
                <li>work in parallel</li>
                <li>crawlers should fetch sites near them -> <b>locality awareness</b></li>
              </ul>
          </ul>
        </section>

        <section>
          <h2>What is Crawl.js</h2>
          <ul>
            <li>distributed &amp; decentralized crawler</li>
            <li>locality aware</li>
            <li>modular</li>
          </ul>
        </section>

        <section>
          <h2>What Crawl.js is <b>not</b></h2>
          <ul>
            <li>search engine</li>
          </ul>
        </section>

        <section>
          <h2>Decentralized</h2>
          <p>
            All crawlers are equal and independant from each other.
          </p>

          <aside class="notes">
            - distributed
            - No central managing unit
          </aside>
        </section>

        <section>
          <h2>Node.js</h2>
          <q cite="http://nodejs.org/">
            Node.js is a platform built on Chrome's JavaScript runtime for easily building fast, scalable network applications. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight and efficient, perfect for data-intensive real-time applications that run across distributed devices.
          </q>
          <aside class="notes">
            - JavaScript with OS api
          </aside>
        </section>


        <!-- architecture -->
        <section>
          <h2>Architecture</h2>
          <p class="fragment">Workers</p>
          <p class="fragment">Queues</p>
          <p class="fragment">Stores</p>
        </section>

        <section data-background="img/system_overview.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Queue</h2>
          <ul>
            <li>a sorted set</li>
            <li>backed by redis - an advanced key-value store.</li>
            <li>identified by a key. Ex. <b>urls:1</b></li>
            <!--
            <li>multiple producers (all workers)</li>
            <li>atomic Test-and-set operation <b>required</b></li>
            -->
          </ul>
        </section>

        <section data-background="img/system_overview.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Worker</h2>
          <ul>
            <li>Startup arguments: <b>url-block</b></li>
            <li>Fetch pages concurrently (configurable)</li>
            <li>Parse &amp; store content</li>
            <li>Independent &amp; autonomic (local queue)</li>
            <li>depends on <b>mapper</b></li>
          </ul>
          <aside class="notes">
            - At any given time a worker is able to map an URL to an identifier <b>independently</b>
          </aside>
        </section>

        <section data-background="img/system_overview.svg" data-background-size="60%">
        </section>

        <!-- mapper -->
        <section>
          <h3>Mapper</h3>
          <ul>
            <li>a hash function. (currently MD5)</li>
            <li>maps any URL to an <b>id</b> E {0,1,..,#workers}</li>
            <li>all URLs with the same <b>id</b> belong to one <b>block</b></li>
            <li class="fragment">no communication needed!</li>
            <li class="fragment">makes the system <b>decentralized</b></li>
          </ul>
        </section>

        <section data-background="img/system_overview.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Store</h2>
          <ul>
            <li>Persist downloaded content (html/images)</li>
            <li>Implementations: Filesystem, cassandra, riak, dummy</li>
          </ul>
        </section>

        <section>
          <h2>Locality awareness</h2>
          <ul>
            <li>Goal: crawlers should fetch sites near them (in terms of latency)</li>
          </ul>
        </section>

        <section>
          <h2>Locality awareness cont.</h2>
          <ul>
            <li>Crawl.js supports multiple <b>groups</b> of workers</li>
            <li>a group can be a datacenter or a region. (configurable)</li>
            <li>implemented in <b>mapper</b></li>
          </ul>
        </section>

        <section>
          <h2>Worker (with groups)</h2>
          <ul>
            <li>a worker belongs to one <b>group</b></li>
            <li>withing that group he is responsible for one <b>block</b></li>
            <li>Startup arguments: <b>group</b> <b>block</b></li>
          </ul>
        </section>

        <section>
          <h2>Queue (with groups)</h2>
          <ul>
            <li>a sorted set in redis</li>
            <li>identified by key <b>group:0:block:1</b>.</li>
          </ul>
        </section>


        <section>
          <h1>Experiments &amp; results</h1>
        </section>

        <section>
          <h2>Experiments &amp; Results</h2>
          <ul>
            <li>Closed environment (OpenNebula cluster)</li>
            <li>Analyse scaling behaviour (measure <b>pages/second</b>)</li>
            <li>Demonstrate locality awareness</li>
          </ul>
        </section>

        <section>
          <h2>Scaling / Performance</h2>
          <ul>
            <li>network topology without latencies</li>
            <li>1, 2, 3, 4, and 5 workers</li>
          </ul>
        </section>

        <section data-background="img/topology_exp1.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Scaling / Performance cont.</h2>
          <canvas id="chart_exp1" width="400" height="400"></canvas>
          <p class="small">Workers</p>
        </section>

        <section>
          <h2>Scaling / Performance cont.</h2>
          <ul>
            <li class="fragment">worker vm: 1 CPU, 1GB RAM</li>
            <li class="fragment">~ 250 pages / worker &amp; second</li>
            <li class="fragment">almost linear scaling</li>
          </ul>
          <aside class="notes">
            - with parsing (https://github.com/fb55/htmlparser2) sax, supports streaming
          </aside>
        </section>

        <section>
          <h2>Locality awareness</h2>
          <ul>
            <li>4 workers</li>
            <li>150 ms latency to cross-region sites</li>
            <li>2 configurations</li>
          </ul>
        </section>

        <section data-background="img/topology_exp2.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Configuration 1 (1 group)</h2>
          <ul>
            <li><b>group 0</b> fetches from both sites</li>
            <li>cross-region crawls expected</li>
          </ul>
        </section>

        <section data-background="img/topology_exp3.svg" data-background-size="60%">
        </section>

        <section>
          <h2>Configuration 2 (2 groups)</h2>
          <ul>
            <li><b>group 0</b> -> www0 (bn.wikipedia.org)</li>
            <li><b>group 1</b> -> www1 (bs.wikipedia.org)</li>
            <li><b>no</b> cross-region crawls expected</li>
          </ul>
        </section>

        <section>
          <h2>Locality awareness cont.</h2>
          <canvas id="chart_exp2" width="600" height="400"></canvas>
        </section>

        <section>
          <h2>Locality awareness cont.</h2>
          <ul>
            <li>restored performance by regrouping workers</li>
            <li>Crawl.js can be configured to respect locality</li>
            <li>successful experiments</li>
          </ul>
        </section>

        <section>
          <h2>Conclusions</h2>
          <ul>
            <li>good linear scaling</li>
            <li>fundation to build upon</li>
            <li>simple/small code base (~1K LOC)</li>
          </ul>
        </section>

        <section>
          <h2>Future work</h2>
          <ul>
            <li>Dynamic configuration</li>
            <li>Monitoring / administration (public crawl)</li>
            <li>Indexing, page-importance algorithms, ..</li>
          </ul>
        </section>

        <section>
          <h1>THE END</h1>
          <h3>http://www.github.com/crawljs</h3>
          <p>Author: Cédric Reginster</p>
          <p>Supervisor: Prof. Pascal Felber</p>
        </section>

        <!-- backup -->
        <section>
          <h1>Backup</h1>
        </section>

        <section>
          <h2>Mapper (with groups)</h2>
          <ul>
            <li>http://en.wikipedia.org/wiki/Web_crawler</li>
            <li>'en.wikipedia.org' &#8594; <b>group</b></li>
            <li>'/wiki/Web_crawler' &#8594; <b>block</b> within that group</li>
          </ul>
        </section>

        <section>
          <h2>CLOC</h2>
          <pre><code data-trim style="font-size: 16px; line-height: 1.2;">
$ cloc crawl.js/ --exclude-dir=node_modules
      30 text files.
      30 unique files.                              
    1179 files ignored.

http://cloc.sourceforge.net v 1.60  T=0.23 s (109.3 files/s, 7363.7 lines/s)
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Javascript                      25            374            251           1059
-------------------------------------------------------------------------------
SUM:                            25            374            251           1059
-------------------------------------------------------------------------------
          </code></pre>
        </section>

        <section>
          <pre><code data-trim style="font-size: 16px; line-height: 1.2;">
/* 
 * map a key (string) to a number between 0 and limit - 1.
 * Bacause we do the modulo operation on the last byte
 * (of md5 hash) the max. range is [0,127]
 */
Mapper.prototype._map = function (key, limit) {

  if (!key) { throw new Error('Missing `key` to map!!'); }

  var hash = crypto.createHash('md5')
    , buf;

  hash.update(key, 'ascii');

  buf = hash.digest();

  return buf[buf.length - 1] % limit;

};
          </code></pre>
      </section>

    </div>

  </div>

  <script src="lib/js/head.min.js"></script>
  <script src="lib/js/Chart.min.js"></script>
  <script src="lib/js/charts.js"></script>
  <script src="js/reveal.min.js"></script>

  <script>

    // Full list of configuration options available here:
    // https://github.com/hakimel/reveal.js#configuration
  Reveal.initialize({
    controls: true,
    progress: true,
    history: true,
    center: true,

    theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
    transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

    // Parallax scrolling
    // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
    // parallaxBackgroundSize: '2100px 900px',

    // Optional libraries used to extend on reveal.js
    dependencies: [
  { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
    { src: 'plugin/math/math.js', async: true },
    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
  ]
  });

</script>

</body>
</html>
